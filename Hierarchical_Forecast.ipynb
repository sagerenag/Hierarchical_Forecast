{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagerenag/Hierarchical_Forecast/blob/main/Hierarchical_Forecast.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importante\n",
        "\n",
        "este notebook esta pensado para ejecutarse en google colab, se recomienda tambien tener en su unidad los csv usados en este codigo o por lo menos, unos csv que cumplan con las caracteristicas esperadas en la data para evitar cualquier inconveniente con la ejecucion del mismo"
      ],
      "metadata": {
        "id": "gd8rILxXnsmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pipelines"
      ],
      "metadata": {
        "id": "U16fIpsM8bxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar librerias necesarias\n",
        "!pip install hierarchicalforecast\n",
        "!pip install ydata-profiling\n",
        "!pip install statsforecast\n",
        "!pip install mlforecast neuralforecast\n",
        "!pip install xgboost\n",
        "!pip install datasetsforecast\n",
        "!pip install imbalanced-learn\n",
        "\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import product\n",
        "from imblearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "# Import additional libraries\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import  FunctionTransformer\n",
        "from sklearn import preprocessing as prep\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "\n",
        "# Import forecasting libraries\n",
        "from hierarchicalforecast.core import HierarchicalReconciliation\n",
        "from hierarchicalforecast.evaluation import HierarchicalEvaluation\n",
        "from hierarchicalforecast.methods import OptimalCombination, MinTrace, BottomUp\n",
        "from hierarchicalforecast.utils import aggregate\n",
        "from statsforecast.core import StatsForecast\n",
        "from statsforecast.models import DynamicOptimizedTheta as DOT\n",
        "from mlforecast import MLForecast\n",
        "from neuralforecast import NeuralForecast\n",
        "from neuralforecast.auto import AutoLSTM\n",
        "from xgboost import XGBRegressor\n",
        "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
        "from datasetsforecast.losses import rmse\n",
        "\n",
        "# cargar los datos en csv\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load data from CSV files\n",
        "productos = pd.read_csv('/content/drive/MyDrive/Tabla_Maestra_Productos.csv')\n",
        "Ventas = pd.read_csv('/content/drive/MyDrive/Ventas.csv')\n",
        "\n",
        "\n",
        "# Define a function to aggregate hierarchies in the data\n",
        "def aggregate_hierarchies(df, column):\n",
        "    hierarchies = [['Pais'], ['Pais', 'Proyecto'], ['Pais', 'Proyecto', 'SnkProducto']]\n",
        "    df, H_df, tags = aggregate(\n",
        "    df[['SnkFecha', 'SnkProducto', column, 'Pais', 'Proyecto']].rename(columns={'SnkFecha':'ds', column:'y'}),\n",
        "    spec = hierarchies\n",
        "    )\n",
        "    df = df.reset_index()\n",
        "    return(df, H_df, tags)\n",
        "\n",
        "# Define a function to filter product data\n",
        "def filter_producto(productos):\n",
        "    productos = productos[[\"SnkProducto\", 'Codigo']]\n",
        "    productos['Codigo'] = productos['Codigo'].astype(str)\n",
        "    # get final products\n",
        "    filtro = pd.read_excel(\"/content/drive/MyDrive/filtro.xlsx\", header=1)\n",
        "    filtro = filtro['SAP'].tolist()\n",
        "    filtro = list(map(str, filtro))\n",
        "    #apply filter to products\n",
        "    productos = productos[productos['Codigo'].isin(filtro)].drop(['Codigo'], axis=1)\n",
        "    return productos\n",
        "\n",
        "# Define a function to subset sales data\n",
        "def subset_ventas(df):\n",
        "    return df[['SnkFecha', 'SnkProducto', 'Unidades', 'Proyecto', 'NumeroDocumento']]\n",
        "\n",
        "# Define a function to remove duplicates in sales data\n",
        "def eliminar_duplicados(df):\n",
        "    return df.drop_duplicates(subset=['NumeroDocumento', 'Unidades', 'SnkProducto'])\n",
        "\n",
        "# Define a function to select specific columns in sales data\n",
        "def seleccionar_columnas(df):\n",
        "    df['SnkProducto'] = df['SnkProducto'].astype(str)\n",
        "    return df[['SnkFecha', 'SnkProducto', 'Unidades', 'Proyecto']]\n",
        "\n",
        "# Define a function to filter sales with positive units\n",
        "def filtrar_ventas_positivas(df):\n",
        "    return df[df['Unidades'] > 0]\n",
        "\n",
        "# Define a function to calculate sold volume for each product\n",
        "def calcular_volumen_vendido(df, productos):\n",
        "    productos['SnkProducto'] = productos['SnkProducto'].astype(str)\n",
        "    merged_df = df.merge(productos, on=\"SnkProducto\", how=\"inner\")\n",
        "    return merged_df\n",
        "\n",
        "# Define a function to process dates in sales data\n",
        "def procesar_fechas(df):\n",
        "    df['SnkFecha'] = pd.to_datetime(df['SnkFecha'], format=\"%Y%m%d\") - pd.to_timedelta(7, unit='d')\n",
        "    df = df.groupby([pd.Grouper(key='SnkFecha', freq='W'), 'Proyecto', 'SnkProducto'])['Unidades'].sum().reset_index()\n",
        "    return df\n",
        "\n",
        "# Define a function to generate combinations of dates, products, and cities\n",
        "def generar_combinaciones(df):\n",
        "    fechas = df['SnkFecha'].unique()\n",
        "    productos = df['SnkProducto'].unique()\n",
        "    ciudades = df['Proyecto'].unique()\n",
        "    combinaciones = list(product(fechas, ciudades, productos))\n",
        "    combinaciones_df = pd.DataFrame(combinaciones, columns=['SnkFecha', 'Proyecto', 'SnkProducto'])\n",
        "    return df, combinaciones_df\n",
        "\n",
        "# Define a function to combine dataframes\n",
        "def combinar_dataframes(df, combinaciones_df):\n",
        "    return combinaciones_df.merge(df, on=['SnkFecha', 'Proyecto', 'SnkProducto'], how='left')\n",
        "\n",
        "# Define a function to fill null values in sales data\n",
        "def llenar_valores_nulos(df):\n",
        "    df['Unidades'].fillna(0, inplace=True)\n",
        "    return df\n",
        "\n",
        "# Define a function to add a 'Pais' category to sales data\n",
        "def agregar_categoria_pais(df):\n",
        "    df['Pais'] = \"Colombia\"\n",
        "    return df\n",
        "\n",
        "# Define a function for preprocessing sales data\n",
        "def preprocessing(Ventas1, productos):\n",
        "    productos = filter_producto(productos)\n",
        "    steps = [\n",
        "        ('subset_ventas', FunctionTransformer(subset_ventas)),\n",
        "        ('eliminar_duplicados', FunctionTransformer(eliminar_duplicados)),\n",
        "        ('seleccionar_columnas', FunctionTransformer(seleccionar_columnas)),\n",
        "        ('filtrar_ventas_positivas', FunctionTransformer(filtrar_ventas_positivas)),\n",
        "        ('calcular_volumen_vendido', FunctionTransformer(calcular_volumen_vendido, kw_args={'productos': productos})),\n",
        "        ('procesar_fechas', FunctionTransformer(procesar_fechas)),\n",
        "        ('generar_combinaciones', FunctionTransformer(generar_combinaciones))\n",
        "      ]\n",
        "    pipeline_1 = Pipeline(steps)\n",
        "    Final, combinaciones_df = pipeline_1.fit_transform(Ventas1)\n",
        "    # Aplicar las funciones en orden para crear el pipeline\n",
        "    steps_2 = [\n",
        "        ('combinar_dataframes', FunctionTransformer(combinar_dataframes, kw_args={'combinaciones_df': combinaciones_df})),  # Use custom transformer\n",
        "        ('llenar_valores_nulos', FunctionTransformer(llenar_valores_nulos)),\n",
        "        ('agregar_categoria_pais', FunctionTransformer(agregar_categoria_pais)),\n",
        "    ]\n",
        "\n",
        "    pipeline_2 = Pipeline(steps_2)\n",
        "\n",
        "    # Apply the pipeline to the Ventas1 dataframe\n",
        "    Ventas1_processed = pipeline_2.fit_transform(Final)\n",
        "\n",
        "\n",
        "    Final_vol, H_vol, tags_vol = aggregate_hierarchies(Ventas1_processed, 'Unidades')\n",
        "    return Final_vol, H_vol, tags_vol\n",
        "\n",
        "# Define a function to split data for volume forecasting\n",
        "def split_data_vol(resultado_vol):\n",
        "    test = resultado_vol.groupby('unique_id').tail(26)\n",
        "    lbl = prep.LabelEncoder()\n",
        "    test['unique_id'] = lbl.fit_transform(test['unique_id'].astype(str))\n",
        "    test['ds'] = lbl.fit_transform(test['ds'].astype(str))\n",
        "\n",
        "    train = resultado_vol.drop(test.index)\n",
        "    train['unique_id'] = lbl.fit_transform(train['unique_id'].astype(str))\n",
        "    train['ds'] = lbl.fit_transform(train['ds'].astype(str))\n",
        "\n",
        "    return train, test\n",
        "\n",
        "def objective(space):\n",
        "    clf=XGBRegressor(\n",
        "                    n_estimators =space['n_estimators'],\n",
        "                    max_depth = int(space['max_depth']),\n",
        "                    gamma = space['gamma'],\n",
        "                    reg_alpha = int(space['reg_alpha']),\n",
        "                    min_child_weight=int(space['min_child_weight']),\n",
        "                    colsample_bytree=int(space['colsample_bytree']),\n",
        "                    eval_metric=mean_absolute_error)\n",
        "\n",
        "    evaluation = [ ( test_vol.drop(columns=['y']), test_vol.y)]\n",
        "\n",
        "    clf.fit(train_vol.drop(columns=['y']), train_vol.y,\n",
        "            eval_set=evaluation,\n",
        "            early_stopping_rounds=10,verbose=False)\n",
        "\n",
        "\n",
        "    pred = clf.predict(test_vol.drop(columns=['y']))\n",
        "    RMSE = rmse(test_vol.y, pred)\n",
        "    print (\"RMSE:\", RMSE)\n",
        "    return {'loss': RMSE, 'status': STATUS_OK }\n",
        "\n",
        "def cross_validation_sf(resultado_vol, seasons, types):\n",
        "    sf_Vol = {}\n",
        "    for i in seasons:\n",
        "        print(i)\n",
        "        for k in types:\n",
        "            print(k)\n",
        "            sf_Volumen = StatsForecast(\n",
        "                df=resultado_vol,\n",
        "                models=[DOT(season_length=i, decomposition_type=k)],\n",
        "                freq='W',\n",
        "                n_jobs=-1\n",
        "            )\n",
        "            cross_volumen = sf_Volumen.cross_validation(\n",
        "                df=resultado_vol,\n",
        "                h=4,\n",
        "                step_size=4,\n",
        "                n_windows=12\n",
        "            )\n",
        "            sf_Vol[rmse(cross_volumen['y'], cross_volumen['DynamicOptimizedTheta'])] = [i, k]\n",
        "\n",
        "    return sf_Vol\n",
        "def optimize_hyperparameters(space):\n",
        "    trials = Trials()\n",
        "\n",
        "    best_hyperparams = fmin(fn = objective,\n",
        "                        space = space,\n",
        "                        algo = tpe.suggest,\n",
        "                        max_evals = 100,\n",
        "                        trials = trials)\n",
        "\n",
        "    return best_hyperparams\n",
        "def split_data(data):\n",
        "    test_data = data.groupby('unique_id').tail(8)\n",
        "    train_data = data.drop(test_data.index)\n",
        "    test_data = test_data.set_index('unique_id')\n",
        "    train_data = train_data.set_index('unique_id')\n",
        "    return train_data, test_data\n",
        "\n",
        "def create_stats_forecast(train_data, sf_params):\n",
        "    sf_object = StatsForecast(\n",
        "        df=train_data,\n",
        "        models=[DOT(season_length=sf_params[min(sf_params)][0], decomposition_type=sf_params[min(sf_params)][1])],\n",
        "        freq='W',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    return sf_object\n",
        "\n",
        "def create_ml_forecast(df, ml_params):\n",
        "    mlf = MLForecast(\n",
        "        models=[XGBRegressor(\n",
        "            max_depth=int(ml_params['max_depth']),\n",
        "            gamma=ml_params['gamma'],\n",
        "            reg_alpha=ml_params['reg_alpha'],\n",
        "            min_child_weight=ml_params['min_child_weight'],\n",
        "            colsample_bytree=ml_params['colsample_bytree'],\n",
        "            reg_lambda=ml_params['reg_alpha']\n",
        "        )],\n",
        "        freq='W',\n",
        "        lags=list(range(1, 53))\n",
        "    )\n",
        "    return mlf\n",
        "\n",
        "def create_neural_forecast(df):\n",
        "    nf = NeuralForecast(\n",
        "        models=[AutoLSTM(h=74, config=None, backend='optuna')],\n",
        "        freq='W'\n",
        "    )\n",
        "    return nf\n",
        "\n",
        "def fit_ml_forecast(mlf, train_data, ho):\n",
        "    fit_ml = mlf.fit(train_data.reset_index())\n",
        "    forecasts_ml = fit_ml.predict(h=ho).set_index('unique_id')\n",
        "    return mlf, forecasts_ml\n",
        "\n",
        "def fit_neural_forecast(nf, train_data):\n",
        "    nf.fit(df=train_data.reset_index())\n",
        "    forecast_nn = nf.predict()\n",
        "    forecast_nn_test = forecast_nn.groupby('unique_id').head(8)\n",
        "    return forecast_nn, forecast_nn_test\n",
        "\n",
        "def forecast_stats_volume(sf_object, ho):\n",
        "    forecasts_stats = sf_object.forecast(h=ho)\n",
        "    return sf_object, forecasts_stats\n",
        "\n",
        "def merge_forecasts(stats_forecast, ml_forecast, neural_forecast):\n",
        "    forecasts_volume = stats_forecast.merge(ml_forecast, on=['unique_id', 'ds']).merge(neural_forecast, on=['unique_id', 'ds'])\n",
        "    return forecasts_volume\n",
        "\n",
        "def reconcile_forecasts(forecasts_volume, train_data, h_volume, tags_volume):\n",
        "    hrec = HierarchicalReconciliation(\n",
        "        reconcilers=[\n",
        "            MinTrace(method='ols', nonnegative=True),\n",
        "            OptimalCombination(method='wls_struct', nonnegative=True),\n",
        "            BottomUp()\n",
        "        ]\n",
        "    )\n",
        "    forecasts_rec_volume = hrec.reconcile(Y_hat_df=forecasts_volume, Y_df=train_data, S=h_volume, tags=tags_volume)\n",
        "    return forecasts_rec_volume\n",
        "\n",
        "def mean_squared_scaled_error(y, y_hat):\n",
        "    mse = np.mean((y - y_hat) ** 2)\n",
        "    variance = np.var(y)\n",
        "    msse = round(mse / variance, 3)\n",
        "    return msse\n",
        "\n",
        "def cumulative_accuracy(y, y_hat):\n",
        "    sum_predictions = np.sum(y_hat)\n",
        "    sum_actual = np.sum(y)\n",
        "    if sum_predictions > sum_actual:\n",
        "        accuracy = round((sum_actual / sum_predictions) * 100, 2)\n",
        "    else:\n",
        "        accuracy = round((sum_predictions / sum_actual) * 100, 2)\n",
        "    return accuracy\n",
        "\n",
        "def mean_squared_error(y, y_hat):\n",
        "    mse_value = round(np.mean((np.array(y) - np.array(y_hat)) ** 2), 2)\n",
        "    return mse_value\n",
        "\n",
        "def hierarchical_evaluation(forecasts_rec_volume, test_data, tags_volume):\n",
        "    evaluator = HierarchicalEvaluation(evaluators=[mean_squared_scaled_error, cumulative_accuracy, mean_squared_error])\n",
        "    vol_eval = evaluator.evaluate(Y_hat_df=forecasts_rec_volume, Y_test_df=test_data, tags=tags_volume).astype(float)\n",
        "    msse_rows = vol_eval.index.get_level_values('metric').isin(['mean_squared_scaled_error', 'mean_squared_error'])\n",
        "    c_acc_rows = ~msse_rows\n",
        "    vol_eval['best_model'] = np.where(msse_rows, vol_eval.apply(lambda row: row.idxmin(), axis=1),\n",
        "                                      vol_eval.apply(lambda row: row.idxmax(), axis=1))\n",
        "    return vol_eval\n",
        "def save_and_download_evaluation(vol_eval, filename='eval_vol.csv'):\n",
        "    vol_eval.to_csv(filename, encoding='utf-8-sig')\n",
        "    files.download(filename)\n",
        "    return vol_eval\n",
        "def evaluate_model(resultado_vol, best_hyperparams, H_vol, tags_vol, sf_Vol):\n",
        "    print(\"fiting models\")\n",
        "    train_volume, test_volume = split_data(resultado_vol)\n",
        "    steps = [\n",
        "        ('create_stats_forecast', FunctionTransformer(create_stats_forecast, kw_args={'sf_params': sf_Vol})),  # Use custom transformer\n",
        "        ('forecast_stats_volume', FunctionTransformer(forecast_stats_volume, kw_args={'ho': 8}))\n",
        "      ]\n",
        "    pipeline_1 = Pipeline(steps)\n",
        "    sf_volume, forecasts_stats_volume = pipeline_1.fit_transform(train_volume)\n",
        "    # Aplicar las funciones en orden para crear el pipeline\n",
        "    steps_1 = [\n",
        "        ('create_ml_forecast', FunctionTransformer(create_ml_forecast, kw_args={'ml_params':best_hyperparams })),  # Use custom transformer\n",
        "        ('fit_ml_forecast', FunctionTransformer(fit_ml_forecast, kw_args={'train_data':train_volume , 'ho': 8}))\n",
        "      ]\n",
        "    pipeline_2 = Pipeline(steps_1)\n",
        "    ml_forecast, forecasts_ml_volume = pipeline_2.fit_transform(train_volume)\n",
        "    # Aplicar las funciones en orden para crear el pipeline\n",
        "    steps_2 = [\n",
        "        ('create_neural_forecast', FunctionTransformer(create_neural_forecast)),  # Use custom transformer\n",
        "        ('fit_neural_forecast', FunctionTransformer(fit_neural_forecast, kw_args={'train_data':train_volume}))\n",
        "      ]\n",
        "    pipeline_3 = Pipeline(steps_2)\n",
        "    forecast_nn_volume, forecast_nn_test_volume = pipeline_3.fit_transform(train_volume)\n",
        "    print(\"hierarchical compensation and evaluation\")\n",
        "    steps_3 = [\n",
        "        ('create_neural_forecast', FunctionTransformer(merge_forecasts, kw_args={'ml_forecast':forecasts_ml_volume, 'neural_forecast': forecast_nn_test_volume})),  # Use custom transformer\n",
        "        ('reconcile_forecasts', FunctionTransformer(reconcile_forecasts, kw_args={'train_data':train_volume, 'h_volume':H_vol, 'tags_volume': tags_vol})),\n",
        "        ('hierarchical_evaluation', FunctionTransformer(hierarchical_evaluation, kw_args={'test_data':test_volume, 'tags_volume':tags_vol})),  # Use custom transformer\n",
        "        ('save_and_download_evaluation', FunctionTransformer(save_and_download_evaluation))\n",
        "      ]\n",
        "    pipeline_4 = Pipeline(steps_3)\n",
        "    evaluation_volume = pipeline_4.fit_transform(forecasts_stats_volume)\n",
        "    return forecast_nn_volume, ml_forecast, sf_volume, train_volume, evaluation_volume\n",
        "\n",
        "def process_best_models(vol_eval, tags_vol):\n",
        "    vol_eval.reset_index(inplace=True)\n",
        "    best_model_acc_vol = vol_eval[vol_eval['metric'] == 'cumulative_accuracy'][['level', 'best_model']].set_index('level').to_dict()['best_model']\n",
        "    best_model_acc_vol['Pais'] = best_model_acc_vol['Overall']\n",
        "    best_model_acc_vol['Pais/Proyecto'] = best_model_acc_vol['Overall']\n",
        "    best_model_acc_vol['Pais/Proyecto/SnkProducto'] = best_model_acc_vol['Overall']\n",
        "    best_model_acc_vol.pop('Overall')\n",
        "\n",
        "    return best_model_acc_vol\n",
        "def reconcile_forecasts_and_post_process(forecasts_volumen, train_Vol, H_vol, tags_vol):\n",
        "    hrec = HierarchicalReconciliation(\n",
        "        reconcilers=[\n",
        "            MinTrace(method='ols', nonnegative=True),\n",
        "            OptimalCombination(method='wls_struct', nonnegative=True),\n",
        "            BottomUp()\n",
        "        ]\n",
        "    )\n",
        "    forecasts_rec_volumen = hrec.reconcile(Y_hat_df=forecasts_volumen, Y_df=train_Vol, S=H_vol, tags=tags_vol)\n",
        "\n",
        "    final_vol = pd.DataFrame(columns=['unique_id', 'ds', 'prediction'])\n",
        "    forecasts_rec_volumen.reset_index(inplace=True)\n",
        "\n",
        "    for key in tags_vol.keys():\n",
        "        copi1 = forecasts_rec_volumen[forecasts_rec_volumen['unique_id'].isin(tags_vol[key])][['unique_id', 'ds', best_model_acc_vol[key]]]\n",
        "        copi1.rename(columns={best_model_acc_vol[key]: 'prediction'}, inplace=True)\n",
        "        final_vol = final_vol.append(copi1, ignore_index=True)\n",
        "\n",
        "    final_vol['ds'] = pd.to_datetime(final_vol['ds'])\n",
        "\n",
        "    # Group by 'unique_id' and 'ds' (year-month) and apply different aggregation functions\n",
        "    final_vol = final_vol.groupby(['unique_id', final_vol['ds'].dt.to_period(\"M\")]).agg({\n",
        "        'prediction': 'sum'\n",
        "    }).reset_index()\n",
        "\n",
        "    final_vol['prediction'] = round(final_vol['prediction'], 0).astype(int)\n",
        "\n",
        "    return final_vol\n",
        "def process_and_download_predictions(final_vol, resultado_vol, tags_vol, productos):\n",
        "    resultado_vol['ds'] = pd.to_datetime(resultado_vol['ds'])\n",
        "\n",
        "    # Group by 'unique_id' and 'ds' (year-month) and apply different aggregation functions\n",
        "    resultado_vol = resultado_vol.groupby(['unique_id', resultado_vol['ds'].dt.to_period(\"M\")]).agg({\n",
        "        'y': 'sum'\n",
        "    }).reset_index()\n",
        "    resultado_vol.rename(columns={'y': 'real'}, inplace=True)\n",
        "    resultado_vol['real'] = round(resultado_vol['real'], 0).astype(int)\n",
        "\n",
        "    for key in tags_vol.keys():\n",
        "        copi1 = resultado_vol[resultado_vol['unique_id'].isin(tags_vol[key])]\n",
        "        b_vol = final_vol[final_vol['unique_id'].isin(tags_vol[key])]\n",
        "        merge_vol = pd.merge(copi1, b_vol, on=['unique_id', 'ds'], how='outer')\n",
        "        merge_vol = merge_vol.sort_values(['unique_id', 'ds'])\n",
        "        merge_vol[['Año', 'Mes']] = merge_vol['ds'].astype(str).str.split('-', expand=True)\n",
        "        merge_vol.drop(columns=['ds'], inplace=True)\n",
        "\n",
        "        if key == 'Pais/Proyecto':\n",
        "            merge_vol[['Pais', 'Ciudad']] = merge_vol['unique_id'].str.split('/', expand=True)\n",
        "            merge_vol.drop(columns=['unique_id'], inplace=True)\n",
        "        elif key == 'Pais/Proyecto/SnkProducto':\n",
        "            merge_vol[['Pais', 'Ciudad', 'Producto']] = merge_vol['unique_id'].str.split('/', expand=True)\n",
        "            productos['SnkProducto'] = productos['SnkProducto'].astype(str)\n",
        "            merge_vol = merge_vol.merge(productos[['ClaseContable', 'SnkProducto', 'Volumen']],\n",
        "                                        left_on='Producto', right_on='SnkProducto', how='left')\n",
        "            merge_vol.rename(columns={'ClaseContable': 'Categoria', 'prediction': 'Pronostico'}, inplace=True)\n",
        "            merge_vol.drop(columns=['unique_id', 'SnkProducto', 'Volumen'], inplace=True)\n",
        "        else:\n",
        "            merge_vol.rename(columns={'unique_id': 'Pais', 'prediction': 'Pronostico'}, inplace=True)\n",
        "\n",
        "        merge_vol.to_csv(f'predicciones_vol_{key.replace(\"/\", \"-\")}.csv', encoding='utf-8-sig')\n",
        "        files.download(f'predicciones_vol_{key.replace(\"/\", \"-\")}.csv')\n",
        "\n",
        "print(\"preprocesamiento\")\n",
        "resultado_vol, H_vol, tags_vol = preprocessing(Ventas, productos)\n",
        "resultado_vol.drop(resultado_vol.groupby('unique_id').tail(3).index, inplace = True)\n",
        "print(\"hyperparameter tunning (DOT)\")\n",
        "seasons = [1, 13, 26, 52]\n",
        "types = ['multiplicative', 'additive']\n",
        "\n",
        "train_vol, test_vol = split_data_vol(resultado_vol)\n",
        "sf_Vol = cross_validation_sf(resultado_vol, seasons, types)\n",
        "\n",
        "print(\"XGBOOST\")\n",
        "space = {'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n",
        "         'gamma': hp.uniform('gamma', 1, 9),\n",
        "         'reg_alpha': hp.quniform('reg_alpha', 40, 180, 1),\n",
        "         'reg_lambda': hp.uniform('reg_lambda', 0, 1),\n",
        "         'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
        "         'min_child_weight': hp.quniform('min_child_weight', 0, 10, 1),\n",
        "         'n_estimators': 180,\n",
        "         'seed': 0}\n",
        "\n",
        "best_hyperparams = optimize_hyperparameters(space)\n",
        "forecast_nn_volume, ml_forecast, sf_volume, train_volume, evaluation_volume = evaluate_model(resultado_vol, best_hyperparams, H_vol, tags_vol, sf_Vol)\n",
        "\n",
        "print(\"making forecast with best model\")\n",
        "best_model_acc_vol = process_best_models(evaluation_volume, tags_vol)\n",
        "_, forecasts_ml_volume = fit_ml_forecast(ml_forecast, train_volume, 74)\n",
        "_, forecasts_stats_volume = forecast_stats_volume(sf_volume, 74)\n",
        "forecasts_volume = merge_forecasts(forecasts_stats_volume, forecasts_ml_volume, forecast_nn_volume)\n",
        "print(\"post process\")\n",
        "final_vol = reconcile_forecasts_and_post_process(forecasts_volume, train_volume, H_vol, tags_vol)\n",
        "process_and_download_predictions(final_vol, resultado_vol, tags_vol, productos)\n"
      ],
      "metadata": {
        "id": "fvBuEYSjIuvO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
